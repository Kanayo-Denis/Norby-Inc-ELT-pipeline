All Data Engineering projects consists of 3 phases Extraction, Transformation, Loading. This is an ELT project.
* Extraction from S3 bucket, 3 files 'E'
All dimension tables are loaded in the data warehouse in if_common schema
* Load the extracted .csv Files to the staging area in the warehouse 'L'
* We are to make 3 transformations for reports 'T'
1. The total number of orders placed on a public holiday every month, for the past year:
Table name : "agg_public_holiday"
Conitions: The total number of orders placed on a public holiday every month, for the past year.
A public holiday is a day with a day_of_the_week number in the range of 1 - 5 and a
working_day value of false.
After your transformation, the derived table agg_public_holiday should be loaded into
the {your_id}_analytics schema, using the below table schema

2. Total number of late shipments:
Table name : "agg_shipments"
Conditions: A late shipment is one with shipment_date greater than or equal to 6 days after the
order_date and delivery_date is NULL Total number of undelivered shipments
An undelivered shipment is one with delivery_date as NULL and shipment_date as NULL
and the current_date 15 days after order_date.

3. The product with the highest reviews:
Table name : "best_performing_product"
Special conditions : Also provide the day it was ordered the most, either that day was a
public holiday, total review points, percentage distribution of the review points, and
percentage distribution of early shipments to late shipments for that particular product.

* Submission phase:
After creating these tables in the staging area on the warehouse, next is to move them to the analytics schema.
We would also like an export of these tables to be loaded into the analytics_export folder on
our data lake. As analytics_export/ {group_id}/best_performing_product.csv
